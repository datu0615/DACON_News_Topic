{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KoBERT_final.ipynb","provenance":[{"file_id":"12hCD7UZ15IxfjW8pNi3uaOq5fGxIxw4X","timestamp":1626014439720},{"file_id":"14qEgegzHqlPAFj8YYtvH79kVzWRSns2Q","timestamp":1625760424590}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c7e5529df9f34ff5a0176fe58ffe4f47":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6b012f2726ba4e9896d4954267392aa4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d778300d3d40461a8ffd7cdb89e69cd5","IPY_MODEL_3a3007d536e04f538f1695c30137a4d1"]}},"6b012f2726ba4e9896d4954267392aa4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d778300d3d40461a8ffd7cdb89e69cd5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9c34a76dd4614b09b4cdcadf321c35e2","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":371391,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":371391,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6fff0099923745269375082204b13fb0"}},"3a3007d536e04f538f1695c30137a4d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1009af7e6b4e4ba8a8eb20add58a5d08","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 371k/371k [00:00&lt;00:00, 784kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_374119db7f3743aab1737f8f5726224b"}},"9c34a76dd4614b09b4cdcadf321c35e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6fff0099923745269375082204b13fb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1009af7e6b4e4ba8a8eb20add58a5d08":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"374119db7f3743aab1737f8f5726224b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1dd514088f0a49f0bced91b438cc47ff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e853ccf840774c36b7e3cac3e5d4a233","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_155cd677f2034b4b9fd596d2f670d5c8","IPY_MODEL_48bb67e7d0a340a78d78f1f7357c82db"]}},"e853ccf840774c36b7e3cac3e5d4a233":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"155cd677f2034b4b9fd596d2f670d5c8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_28c740ee3b274270943a48f030e37d97","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":77779,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":77779,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3dfc5b5d5aea4a6bbfc9a5b5689be88f"}},"48bb67e7d0a340a78d78f1f7357c82db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_621d4311627344908017681f33be2865","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 77.8k/77.8k [00:00&lt;00:00, 177kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_23c8e35ca1aa412b83d59e91f3dbc885"}},"28c740ee3b274270943a48f030e37d97":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3dfc5b5d5aea4a6bbfc9a5b5689be88f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"621d4311627344908017681f33be2865":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"23c8e35ca1aa412b83d59e91f3dbc885":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7d623ad981aa410e86a4645d5829ba79":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_64563a948b84423d99e00c6a335885a6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8b59d30ba1f14f9491415ff4f9cebca2","IPY_MODEL_0481971d7c7e449e82e38e8e75abb65a"]}},"64563a948b84423d99e00c6a335885a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8b59d30ba1f14f9491415ff4f9cebca2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9dd8e197310747969465a17db1efcadd","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":426,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":426,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_71ab082f3550443abfadaceb43a3a950"}},"0481971d7c7e449e82e38e8e75abb65a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_936ad8dd8eda4d4691dedee22dceb276","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 426/426 [00:00&lt;00:00, 606B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c6889a08bb8b4901b24aa8f906cc3544"}},"9dd8e197310747969465a17db1efcadd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"71ab082f3550443abfadaceb43a3a950":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"936ad8dd8eda4d4691dedee22dceb276":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c6889a08bb8b4901b24aa8f906cc3544":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7898f82388c7425b84d5c797961c592f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_446609ec4fd646038ee909ff6249e6e7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_50fce3d2b2ba4e7ba6f97020070943ef","IPY_MODEL_b4ac3e7168404d01bcb834208ffcbb88"]}},"446609ec4fd646038ee909ff6249e6e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"50fce3d2b2ba4e7ba6f97020070943ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ea30c7adda5543f18dfa123e136fc9d2","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":368792146,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":368792146,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_367c8a1d326043a992b3cbdc3b459ddd"}},"b4ac3e7168404d01bcb834208ffcbb88":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c67c6e65b4f64bd18b26e5cb3ecc2708","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 369M/369M [00:06&lt;00:00, 53.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_59aaa73355334edca91900d4799f22ac"}},"ea30c7adda5543f18dfa123e136fc9d2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"367c8a1d326043a992b3cbdc3b459ddd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c67c6e65b4f64bd18b26e5cb3ecc2708":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"59aaa73355334edca91900d4799f22ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5HAji7s04eRe","executionInfo":{"status":"ok","timestamp":1628643636522,"user_tz":-540,"elapsed":357,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}},"outputId":"6b729922-8dfa-428d-b485-8c97dc705130"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9V1puuSQM_BU"},"source":["!pip install hanja\n","!pip install wordcloud\n","!pip install transformers==3.2\n","!pip install tensorflow-addons\n","!pip install konlpy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ntscQjKSowSY"},"source":["# MECAB"]},{"cell_type":"code","metadata":{"id":"YhLqvWQi6dlL"},"source":["! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-953ZZ8_nMz"},"source":["cd Mecab-ko-for-Google-Colab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RuLVd0p__pjb"},"source":["! bash install_mecab-ko_on_colab190912.sh"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Jlv7vxarCB4"},"source":["# IMPORT"]},{"cell_type":"code","metadata":{"id":"BzxIbpTpvr6G"},"source":["import numpy as np\n","import pandas as pd\n","import os\n","import re\n","import json\n","import matplotlib.pyplot as plt\n","%matplotlib inline \n","from wordcloud import WordCloud\n","import seaborn as sns\n","from tqdm import tqdm\n","\n","import hanja\n","from hanja import hangul\n","from konlpy.tag import Mecab\n","mecab = Mecab()\n","\n","from transformers import *\n","\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","from sklearn.metrics import accuracy_score, log_loss\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold\n","from tensorflow.keras.models import clone_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ls5o_-bCEsYE"},"source":["https://github.com/monologg/KoBERT-Transformers 참고하였습니다."]},{"cell_type":"code","metadata":{"id":"3AmgkdPb482G"},"source":["import logging\n","import os\n","import unicodedata\n","from shutil import copyfile\n"," \n","from transformers import PreTrainedTokenizer\n"," \n"," \n","logger = logging.getLogger(__name__)\n"," \n","VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n","                     \"vocab_txt\": \"vocab.txt\"}\n"," \n","PRETRAINED_VOCAB_FILES_MAP = {\n","    \"vocab_file\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n","    },\n","    \"vocab_txt\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n","    }\n","}\n"," \n","PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n","    \"monologg/kobert\": 512,\n","    \"monologg/kobert-lm\": 512,\n","    \"monologg/distilkobert\": 512\n","}\n"," \n","PRETRAINED_INIT_CONFIGURATION = {\n","    \"monologg/kobert\": {\"do_lower_case\": False},\n","    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n","    \"monologg/distilkobert\": {\"do_lower_case\": False}\n","}\n"," \n","SPIECE_UNDERLINE = u'▁'\n"," \n"," \n","class KoBertTokenizer(PreTrainedTokenizer):\n","    \"\"\"\n","        SentencePiece based tokenizer. Peculiarities:\n","            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n","    \"\"\"\n","    vocab_files_names = VOCAB_FILES_NAMES\n","    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n","    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n","    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n"," \n","    def __init__(\n","            self,\n","            vocab_file,\n","            vocab_txt,\n","            do_lower_case=False,\n","            remove_space=True,\n","            keep_accents=False,\n","            unk_token=\"[UNK]\",\n","            sep_token=\"[SEP]\",\n","            pad_token=\"[PAD]\",\n","            cls_token=\"[CLS]\",\n","            mask_token=\"[MASK]\",\n","            **kwargs):\n","        super().__init__(\n","            unk_token=unk_token,\n","            sep_token=sep_token,\n","            pad_token=pad_token,\n","            cls_token=cls_token,\n","            mask_token=mask_token,\n","            **kwargs\n","        )\n"," \n","        # Build vocab\n","        self.token2idx = dict()\n","        self.idx2token = []\n","        with open(vocab_txt, 'r', encoding='utf-8') as f:\n","            for idx, token in enumerate(f):\n","                token = token.strip()\n","                self.token2idx[token] = idx\n","                self.idx2token.append(token)\n"," \n","        self.max_len_single_sentence = self.max_len - 2  \n","        self.max_len_sentences_pair = self.max_len - 3  \n"," \n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                           \"pip install sentencepiece\")\n"," \n","        self.do_lower_case = do_lower_case\n","        self.remove_space = remove_space\n","        self.keep_accents = keep_accents\n","        self.vocab_file = vocab_file\n","        self.vocab_txt = vocab_txt\n"," \n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(vocab_file)\n"," \n","    @property\n","    def vocab_size(self):\n","        return len(self.idx2token)\n"," \n","    def __getstate__(self):\n","        state = self.__dict__.copy()\n","        state[\"sp_model\"] = None\n","        return state\n"," \n","    def __setstate__(self, d):\n","        self.__dict__ = d\n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                           \"pip install sentencepiece\")\n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(self.vocab_file)\n"," \n","    def preprocess_text(self, inputs):\n","        if self.remove_space:\n","            outputs = \" \".join(inputs.strip().split())\n","        else:\n","            outputs = inputs\n","        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n"," \n","        if not self.keep_accents:\n","            outputs = unicodedata.normalize('NFKD', outputs)\n","            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n","        if self.do_lower_case:\n","            outputs = outputs.lower()\n"," \n","        return outputs\n"," \n","    def _tokenize(self, text, return_unicode=True, sample=False):\n","        \"\"\" Tokenize a string. \"\"\"\n","        text = self.preprocess_text(text)\n"," \n","        if not sample:\n","            pieces = self.sp_model.EncodeAsPieces(text)\n","        else:\n","            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n","        new_pieces = []\n","        for piece in pieces:\n","            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n","                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n","                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n","                    if len(cur_pieces[0]) == 1:\n","                        cur_pieces = cur_pieces[1:]\n","                    else:\n","                        cur_pieces[0] = cur_pieces[0][1:]\n","                cur_pieces.append(piece[-1])\n","                new_pieces.extend(cur_pieces)\n","            else:\n","                new_pieces.append(piece)\n"," \n","        return new_pieces\n"," \n","    def _convert_token_to_id(self, token):\n","        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n","        return self.token2idx.get(token, self.token2idx[self.unk_token])\n"," \n","    def _convert_id_to_token(self, index, return_unicode=True):\n","        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n","        return self.idx2token[index]\n"," \n","    def convert_tokens_to_string(self, tokens):\n","        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n","        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n","        return out_string\n"," \n","    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n","        by concatenating and adding special tokens.\n","        A RoBERTa sequence has the following format:\n","            single sequence: [CLS] X [SEP]\n","            pair of sequences: [CLS] A [SEP] B [SEP]\n","        \"\"\"\n","        if token_ids_1 is None:\n","            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        sep = [self.sep_token_id]\n","        return cls + token_ids_0 + sep + token_ids_1 + sep\n"," \n","    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n","        \"\"\"\n","        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n","        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n","        Args:\n","            token_ids_0: list of ids (must not contain special tokens)\n","            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n","                for sequence pairs\n","            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n","                special tokens for the model\n","        Returns:\n","            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n","        \"\"\"\n"," \n","        if already_has_special_tokens:\n","            if token_ids_1 is not None:\n","                raise ValueError(\n","                    \"You should not supply a second sequence if the provided sequence of \"\n","                    \"ids is already formated with special tokens for the model.\"\n","                )\n","            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n"," \n","        if token_ids_1 is not None:\n","            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n","        return [1] + ([0] * len(token_ids_0)) + [1]\n"," \n","    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n","        A BERT sequence pair mask has the following format:\n","        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n","        | first sequence    | second sequence\n","        if token_ids_1 is None, only returns the first portion of the mask (0's).\n","        \"\"\"\n","        sep = [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        if token_ids_1 is None:\n","            return len(cls + token_ids_0 + sep) * [0]\n","        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n"," \n","    def save_vocabulary(self, save_directory):\n","        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n","            to a directory.\n","        \"\"\"\n","        if not os.path.isdir(save_directory):\n","            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n","            return\n"," \n","        # 1. Save sentencepiece model\n","        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n"," \n","        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n","            copyfile(self.vocab_file, out_vocab_model)\n"," \n","        # 2. Save vocab.txt\n","        index = 0\n","        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n","        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n","            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n","                if index != token_index:\n","                    logger.warning(\n","                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n","                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n","                    )\n","                    index = token_index\n","                writer.write(token + \"\\n\")\n","                index += 1\n"," \n","        return out_vocab_model, out_vocab_txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2iXF8Qcru5VX"},"source":["PATH = '/content/drive/MyDrive/news/'\n","\n","\n","train = pd.read_csv(PATH + \"train_data.csv\", error_bad_lines=False  )\n","test = pd.read_csv(PATH + \"test_data.csv\",error_bad_lines=False)\n","submission = pd.read_csv(PATH + \"sample_submission.csv\",error_bad_lines=False)\n","topic_dict = pd.read_csv(PATH + \"topic_dict.csv\",error_bad_lines=False)\n","\n","STOPWORDSPATH =PATH + \"stopwords.txt\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CeteJ2oBJtDQ"},"source":["## preprocessing\n","punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n","punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n","\n","def clean_punc(text, punct, mapping):\n","    for p in mapping:\n","        text = text.replace(p, mapping[p])\n","    \n","    for p in punct:\n","        text = text.replace(p, f' {p} ')\n","    \n","    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n","    for s in specials:\n","        text = text.replace(s, specials[s])\n","    \n","    return text.strip()\n","\n","cleaned_train_corpus = []\n","cleaned_test_corpus = []\n","train.title = train.title.apply(lambda x : hanja.translate(x, 'substitution'))\n","test.title = test.title.apply(lambda x : hanja.translate(x, 'substitution'))\n","\n","for sent in train['title']:\n","    cleaned_train_corpus.append(clean_punc(sent, punct, punct_mapping))\n","    \n","for sent in test['title']:\n","    cleaned_test_corpus.append(clean_punc(sent, punct, punct_mapping))\n","\n","\n","def clean_text(texts):\n","    corpus = []\n","    for i in range(0, len(texts)):\n","        texts[i] = texts[i].replace(\"外人\",\"외국인\")\n","        texts[i] = texts[i].replace(\"日\",\"일본\")\n","        texts[i] = texts[i].replace(\"美\",\"미국\")\n","        texts[i] = texts[i].replace(\"北\",\"북한\")\n","        texts[i] = texts[i].replace(\"英\",\"영국\")\n","        texts[i] = texts[i].replace(\"中\",\"중국\")\n","        texts[i] = texts[i].replace(\"與\",\"여당\")\n","        texts[i] = texts[i].replace(\"靑\",\"청와대\")\n","        texts[i] = texts[i].replace(\"野\",\"야당\")\n","        texts[i] = texts[i].replace(\"伊\",\"이탈리아\")\n","        texts[i] = texts[i].replace(\"韓\",\"한국\")\n","        texts[i] = texts[i].replace(\"南\",\"한국\")\n","        texts[i] = texts[i].replace(\"獨\",\"독일\")\n","        texts[i] = texts[i].replace(\"佛\",\"프랑스\")\n","        texts[i] = texts[i].replace(\"檢\",\"검찰\")\n","        texts[i] = texts[i].replace(\"銀\",\"은행\")\n","        texts[i] = texts[i].replace(\"亞\",\"아시아\")\n","        texts[i] = texts[i].replace(\"人\",\"사람\")\n","        texts[i] = texts[i].replace(\"孫\",\"손혜원\")\n","        texts[i] = texts[i].replace(\"企\",\"기업\")\n","        texts[i] = texts[i].replace(\"前\",\"이전\")\n","        texts[i] = texts[i].replace(\"反\",\"반대\")\n","        texts[i] = texts[i].replace(\"安\",\"안철수\")\n","        texts[i] = texts[i].replace(\"展\",\"전시회\")\n","        texts[i] = texts[i].replace(\"故\",\"사망\")\n","        texts[i] = texts[i].replace(\"文\",\"문재인\")\n","        texts[i] = texts[i].replace(\"新\",\"새로운\")\n","        texts[i] = texts[i].replace(\"曺\",\"조국\")\n","        texts[i] = texts[i].replace(\"朴\",\"박근혜\")\n","        texts[i] = texts[i].replace(\"株\",\"주식\")\n","        texts[i] = texts[i].replace(\"男\",\"남자\")\n","        texts[i] = texts[i].replace(\"硏\",\"연구\")\n","        texts[i] = texts[i].replace(\"車\",\"자동차\")\n","        texts[i] = texts[i].replace(\"軍\",\"군대\")\n","        texts[i] = texts[i].replace(\"重\",\"중공업\")       \n","\n","        review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(texts[i])) #remove punctuation\n","        review = re.sub(r'1보','', str(review))\n","        review = re.sub(r'\\d+','', str(review))# remove number\n","        review = re.sub(r'→','에서 ', str(review))\n","        review = re.sub(r'…',' ', str(review))\n","        review = re.sub(r'NYT','뉴욕 타임스', str(review))\n","        review = re.sub(r'KAIST','카이스트', str(review))\n","        review = re.sub(r'WMO','세계 기상 기구', str(review))\n","        review = re.sub(r'KBL','한국 프로 농구', str(review))\n","        review = re.sub(r'UAE','아랍에미리트', str(review))\n","        review = re.sub(r'EU','유럽 연합', str(review))\n","        review = re.sub(r'NBA','농구 연맹', str(review))\n","        review = re.sub(r'CIA','중앙정보국', str(review))\n","        review = re.sub(r'ECB','유럽 중앙 은행', str(review))\n","        review = re.sub(r'AFC','아시아 축구 연맹', str(review))\n","        review = re.sub(r'ITU','국제전기통신연합', str(review))\n","        review = re.sub(r'MVP','최우수 선수', str(review))\n","        #review = re.sub(r'MB','이명박', str(review))\n","        review = re.sub(r'APEC','아시아 태평량 경제협력체', str(review))\n","        review = re.sub(r'PSG','파리 셍제르망', str(review))\n","        review = re.sub(r'IMO','국제해사기구', str(review))\n","        review = re.sub(r'MLB','프로 야구 리그 ', str(review))\n","        review = re.sub(r'MOU','양해각서', str(review))\n","        review = re.sub(r'FA','자유계약선수제도', str(review))\n","        review = re.sub(r'EPL','잉글랜드프리미어리그', str(review))\n","        review = re.sub(r'KBO','한국야구위원회', str(review))\n","        review = re.sub(r'IPU','국제 의회 연맹', str(review))\n","        review = re.sub(r'AG','아시안게임', str(review))\n","        review = re.sub(r'PS','포스트시즌', str(review))\n","        review = re.sub(r'PO','플레이오프', str(review))\n","        #review = re.sub(r'닷컴','사이트', str(review))\n","        review = re.sub(r'OUT','방출', str(review))\n","        review = re.sub(r'IN','영입', str(review))\n","        review = re.sub(r'TPP',' 환태평양 경제 동반자협정', str(review))\n","        review = re.sub(r'EAS','동아시아 정상회의', str(review))\n","        review = re.sub(r'DC','', str(review))\n","        review = re.sub(r'①','', str(review))\n","        review = re.sub(r'②','', str(review))\n","        review = re.sub(r'⑤','', str(review))\n","        review = re.sub(r'·',' 및 ', str(review))\n","        #sent = re.sub(r'G20','', str(sent))\n","        review = re.sub(r'↑','상승 ', str(review))\n","        review = re.sub(r'↓','하락 ', str(review))\n","        review = re.sub(r'ITF','국제태권도연맹 ', str(review))\n","        review = re.sub(r'IS','이슬람 ', str(review))\n","        review = re.sub(r'러','러시아 ', str(review))\n","        review = re.sub(r'W농구','한국여자농구', str(review))\n","        review = re.sub(r'C팰리스','크리스탈팰리스', str(review))\n","        review = re.sub(r'SLBM','잠수함발사탄도미사일', str(review))\n","        review = re.sub(r'VNL','배구네이션스리그', str(review))\n","        #sent = re.sub(r'D','하루전', str(sent))\n","        review = re.sub(r'LA타임스','로스엔젤레스타임스', str(review))\n","        review = re.sub(r'V리그','배구리그', str(review))\n","        review = re.sub(r'KOVO','한국배구연맹', str(review))\n","        review = re.sub(r'ℓ','리터', str(review))\n","        review = re.sub(r'SUN','선동열', str(review))\n","        review = re.sub(r'WSJ',' 월스트리트 저널', str(review))\n","        review = re.sub(r'ERA',' 평균자책점', str(review))\n","        review = re.sub(r'IoT',' 사물인터넷', str(review))\n","        review = re.sub(r'QS',' 선발 6이닝 이상 3자책점 이하 투구', str(review))\n","        review = re.sub(r'NL','내셔널리그', str(review))\n","        review = re.sub(r'UFG20','한미 합동 군사', str(review))\n","        review = re.sub(r'F35','전투기', str(review))\n","        review = re.sub(r'WP','워싱턴포스트', str(review))\n","        review = re.sub(r'TK','대구와 경북', str(review))\n","        review = re.sub(r'ACL','아시아축구연맹 챔피언스리그', str(review))\n","        review = re.sub(r'IT','정보기술', str(review))\n","        review = re.sub(r'AI','인공지능', str(review))\n","        review = re.sub(r'TF','태스크포스', str(review))\n","        review = re.sub(r'ML','메이저리그', str(review))\n","        review = re.sub(r'FC','축구 클럽', str(review))\n","        review = re.sub(r'SI','스포츠 일러스트레이티드', str(review))\n","        review = re.sub(r'㈜','', str(review))\n","        review = re.sub(r'MS','마이크로소프트', str(review))\n","        review = re.sub(r'SNS','소셜 네트워크 서비스', str(review))\n","        review = re.sub(r'B52','', str(review))\n","        review = re.sub(r'VR','가상현실', str(review))\n","        review = re.sub(r'ELB','주가연계파생결합사채', str(review))\n","        review = re.sub(r'CES','국제전자제품박람회', str(review))\n","        review = re.sub(r'NPL','부실채권', str(review))\n","        review = re.sub(r'IPO','기업공개', str(review))\n","        review = re.sub(r'ERA','방어율', str(review))\n","        review = re.sub(r'MWC','모바일 산업 박람회', str(review))\n","        review = re.sub(r'NSC','국가안전보장회의', str(review))\n","        review = review.lower() #lower case\n","        review = re.sub(r'\\s+', ' ', review) #remove extra space\n","        review = re.sub(r'<[^>]+>','',review) #remove Html tags\n","        review = re.sub(r'\\s+', ' ', review) #remove spaces\n","        review = re.sub(r\"^\\s+\", '', review) #remove space from start\n","        review = re.sub(r'\\s+$', '', review) #remove space from the end\n","        review = re.sub(\"[一-龥]\",'', review)\n","        corpus.append(review)\n","    return corpus\n","\n","basic_preprocessed_train_corpus = clean_text(cleaned_train_corpus)\n","basic_preprocessed_test_corpus = clean_text(cleaned_test_corpus)\n","\n","\n","stopwords = []\n","with open(STOPWORDSPATH) as f:\n","    for line in f:\n","        stopwords.append(line.strip())\n","\n","removed_stopword_train_corpus = []\n","removed_stopword_test_corpus = []\n","\n","for tagged in basic_preprocessed_train_corpus:\n","    tagged=mecab.pos(tagged)\n","    \n","    temp = []\n","    for tag in tagged:\n","        if tag[0] in stopwords or tag[1] not in [\"NNG\", \"NNP\", \"VV\", \"VA\", \"VCP\", \"VCN\", \"MM\", \"MAG\", \"XPN\", \"SL\", \"SH\"]:\n","            continue\n","        temp.append(tag[0])\n","\n","    removed_stopword_train_corpus.append(' '.join(temp))\n","    \n","for tagged in basic_preprocessed_test_corpus:\n","    tagged=mecab.pos(tagged)\n","    \n","    temp = []\n","    for tag in tagged:\n","        if tag[0] in stopwords or tag[1] not in [\"NNG\", \"NNP\", \"VV\", \"VA\", \"VCP\", \"VCN\", \"MM\", \"MAG\", \"XPN\", \"SL\", \"SH\"]:\n","            continue\n","        temp.append(tag[0])\n","\n","    removed_stopword_test_corpus.append(' '.join(temp))\n","\n","\n","train_text = removed_stopword_train_corpus\n","test_text = removed_stopword_test_corpus\n","train_label = np.asarray(train.topic_idx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NKyjChFYvoLu"},"source":["train['clear_title'] = train_text\n","test['clear_title'] = test_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ayZdkh3jwG_6","executionInfo":{"status":"ok","timestamp":1628643981404,"user_tz":-540,"elapsed":15,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}},"outputId":"826953be-5805-4998-8b25-a97911603882"},"source":["train_length = train['clear_title'].astype(str).apply(len)\n","train_length.max()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["51"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"c8eyZF_X1Zzc"},"source":["train.title = clean_text(train.title)\n","test.title = clean_text(test.title)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MPj6TBCou3rO","executionInfo":{"status":"ok","timestamp":1628643987334,"user_tz":-540,"elapsed":17,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}},"outputId":"08177d3e-ac33-4174-d574-7399b4af5e0f"},"source":["train_data_text = list(train['title'])\n","\n","train_clear_text = []\n","\n","for i in tqdm(range(len(train_data_text))):\n","  train_clear_text.append(str(train_data_text[i]).replace('\\\\n', ''))\n","train['clear_title'] = train_clear_text\n","\n","\n","train_clear_text = list(train['clear_title'])\n","\n","train_clear_text2 = []\n","\n","for text in train_clear_text:\n","  temp = re.sub('[-=+,#:;//●<>▲\\?:^$.☆!★()Ⅰ@*\\\"※~>`\\'…》→←]', ' ', text)\n","  train_clear_text2.append(temp)\n","train['clear_title'] = train_clear_text2\n","\n","\n","test_data_text = list(test['title'])\n","\n","test_clear_text = []\n","\n","for i in tqdm(range(len(test_data_text))):\n","  test_clear_text.append(test_data_text[i].replace('\\\\n', ' '))\n","test['clear_title'] = test_clear_text\n","\n","\n","test_clear_text = list(test['clear_title'])\n","\n","test_clear_text_final = []\n","\n","for text in test_clear_text:\n","  temp = re.sub('[-=+,#:;//●<>▲\\?:^$.☆!★()Ⅰ@*\\\"※~>`\\'…》→←]', ' ', text)\n","  test_clear_text_final.append(temp)\n","test['clear_title'] = test_clear_text_final"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 45654/45654 [00:00<00:00, 1148332.58it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210,"referenced_widgets":["c7e5529df9f34ff5a0176fe58ffe4f47","6b012f2726ba4e9896d4954267392aa4","d778300d3d40461a8ffd7cdb89e69cd5","3a3007d536e04f538f1695c30137a4d1","9c34a76dd4614b09b4cdcadf321c35e2","6fff0099923745269375082204b13fb0","1009af7e6b4e4ba8a8eb20add58a5d08","374119db7f3743aab1737f8f5726224b","1dd514088f0a49f0bced91b438cc47ff","e853ccf840774c36b7e3cac3e5d4a233","155cd677f2034b4b9fd596d2f670d5c8","48bb67e7d0a340a78d78f1f7357c82db","28c740ee3b274270943a48f030e37d97","3dfc5b5d5aea4a6bbfc9a5b5689be88f","621d4311627344908017681f33be2865","23c8e35ca1aa412b83d59e91f3dbc885"]},"id":"4bYfpXXE5Tea","executionInfo":{"status":"ok","timestamp":1628643988347,"user_tz":-540,"elapsed":1023,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}},"outputId":"89e05aee-6539-4b45-c595-6857b8328b45"},"source":["tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c7e5529df9f34ff5a0176fe58ffe4f47","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=371391.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1dd514088f0a49f0bced91b438cc47ff","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=77779.0, style=ProgressStyle(descriptio…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1321: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n","Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2YqTZBolw3HL"},"source":["model_name = 'monologg/kobert'\n","SEED_NUM = 615\n","tf.random.set_seed(SEED_NUM)\n","np.random.seed(SEED_NUM)\n","BATCH_SIZE = 128\n","NUM_EPOCHS = 10\n","VALID_SPLIT = 0.2\n","MAX_LEN = 128\n","NUM_CLASS = 7\n","K_SPLIT = 5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p_xHB5wRmoS9"},"source":["def bert_tokenizer(sent, MAX_LEN):\n","\n","    encoded_dict = tokenizer.encode_plus(\n","        \n","        text = sent,\n","        add_special_tokens = True,\n","        max_length = MAX_LEN,\n","        pad_to_max_length = True,                                   \n","        return_attention_mask = True,\n","        truncation = True \n","    )\n","\n","\n","    input_id = encoded_dict['input_ids']\n","    attention_mask = encoded_dict['attention_mask']\n","    token_type_id = encoded_dict['token_type_ids']\n","\n","\n","    return input_id, attention_mask, token_type_id"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s9ECQnRilqoh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628643997610,"user_tz":-540,"elapsed":9267,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}},"outputId":"0ad3eb4c-33e1-40b1-b703-d4d3622364c2"},"source":["input_ids = []\n","attention_masks = []\n","token_type_ids = []\n","train_data_labels = []\n","\n","\n","for train_sent, train_label in tqdm(zip(train[\"clear_title\"], train[\"topic_idx\"])): \n","    try:\n","\n","        input_id, attention_mask, token_type_id = bert_tokenizer(train_sent, MAX_LEN)\n","        \n","        input_ids.append(input_id)\n","        attention_masks.append(attention_mask)\n","        token_type_ids.append(token_type_id)\n","        train_data_labels.append(train_label)\n","        \n","    except Exception as e:\n","        print(e)\n","        print(train_sent)\n","        pass\n","\n","\n","train_news_input_ids = np.array(input_ids, dtype=int)\n","train_news_attention_masks = np.array(attention_masks, dtype=int)\n","train_news_type_ids = np.array(token_type_ids, dtype=int)\n","\n","\n","train_news_inputs = (train_news_input_ids, train_news_attention_masks, train_news_type_ids)\n","train_data_labels = np.asarray(train_data_labels, dtype=np.int32)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","45654it [00:07, 5834.88it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"8xD_SLf4o_rY","colab":{"base_uri":"https://localhost:8080/","height":210,"referenced_widgets":["7d623ad981aa410e86a4645d5829ba79","64563a948b84423d99e00c6a335885a6","8b59d30ba1f14f9491415ff4f9cebca2","0481971d7c7e449e82e38e8e75abb65a","9dd8e197310747969465a17db1efcadd","71ab082f3550443abfadaceb43a3a950","936ad8dd8eda4d4691dedee22dceb276","c6889a08bb8b4901b24aa8f906cc3544","7898f82388c7425b84d5c797961c592f","446609ec4fd646038ee909ff6249e6e7","50fce3d2b2ba4e7ba6f97020070943ef","b4ac3e7168404d01bcb834208ffcbb88","ea30c7adda5543f18dfa123e136fc9d2","367c8a1d326043a992b3cbdc3b459ddd","c67c6e65b4f64bd18b26e5cb3ecc2708","59aaa73355334edca91900d4799f22ac"]},"executionInfo":{"status":"ok","timestamp":1628644007410,"user_tz":-540,"elapsed":9808,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}},"outputId":"4bd84377-e142-485d-8ff6-6ecd48dcdd04"},"source":["class TFBertClassifier(tf.keras.Model):                                                \n","    def __init__(self, model_name, dir_path, num_class):\n","        super(TFBertClassifier, self).__init__()\n","\n","         \n","        self.bert = TFBertModel.from_pretrained(model_name, from_pt=True) \n","                                                                                                                                    \n","        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n","        # self.classifier을 통해 topic_idx를 전부 분류\n","        self.classifier = tf.keras.layers.Dense(num_class,\n","                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range), \n","                                                name=\"classifier\") \n","\n","\n","    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False): \n","        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        pooled_output = outputs[1]\n","        pooled_output = self.dropout(pooled_output, training=training)\n","        logits = self.classifier(pooled_output)\n","\n","        return logits\n","\n","cls_model = TFBertClassifier(model_name=model_name, dir_path='bert_ckpt',num_class=NUM_CLASS)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d623ad981aa410e86a4645d5829ba79","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=426.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7898f82388c7425b84d5c797961c592f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=368792146.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["All PyTorch model weights were used when initializing TFBertModel.\n","\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Jcs1-P5Ot92d"},"source":["optimizer = tfa.optimizers.RectifiedAdam(learning_rate=7.0e-5, total_steps = 2344*4, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-07, clipnorm=1.0)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","\n","cls_model.compile(optimizer=optimizer,\n","                                loss=loss,\n","                                metrics=[metric])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4DvDK6ghupe0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628644007411,"user_tz":-540,"elapsed":12,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}},"outputId":"50a952b8-7c8a-4123-8268-2a50301ec2b5"},"source":["es_callback = EarlyStopping(monitor='val_loss', \n","                                mode='min',\n","                                min_delta=0.0001, \n","                                patience=3,\n","                                baseline=0.4\n","                                 ) \n","\n","DATA_OUT_PATH = '/content/drive/MyDrive/best_model'\n","checkpoint_path = DATA_OUT_PATH +  '/best_modeling.ckpt'\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","if os.path.exists(checkpoint_dir):\n","    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n","else:\n","    os.makedirs(checkpoint_dir, exist_ok=True)\n","    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n","\n","  \n","cp_callback = ModelCheckpoint(\n","    checkpoint_path, \n","    monitor='val_accuracy',\n","    verbose=1, \n","    save_best_only=True, \n","    save_weights_only=True \n","    )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/best_model -- Folder already exists \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d_VxlFjOAC1h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628659944780,"user_tz":-540,"elapsed":15937379,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}},"outputId":"659d3657-4714-4519-85c4-7699976f1f5b"},"source":["history = cls_model.fit(train_news_inputs, train_data_labels, \n","                        epochs=NUM_EPOCHS,\n","                        batch_size=BATCH_SIZE,\n","                        validation_split = VALID_SPLIT,\n","                        callbacks=[es_callback, cp_callback]\n","                        ) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n","286/286 [==============================] - 5287s 18s/step - loss: 1.4119 - accuracy: 0.5055 - val_loss: 0.6953 - val_accuracy: 0.7942\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.79422, saving model to /content/drive/MyDrive/best_model/best_modeling.ckpt\n","Epoch 2/10\n","286/286 [==============================] - 5257s 18s/step - loss: 0.4385 - accuracy: 0.8614 - val_loss: 0.4887 - val_accuracy: 0.8329\n","\n","Epoch 00002: val_accuracy improved from 0.79422 to 0.83288, saving model to /content/drive/MyDrive/best_model/best_modeling.ckpt\n","Epoch 3/10\n","286/286 [==============================] - 5379s 19s/step - loss: 0.3498 - accuracy: 0.8833 - val_loss: 0.4470 - val_accuracy: 0.8481\n","\n","Epoch 00003: val_accuracy improved from 0.83288 to 0.84810, saving model to /content/drive/MyDrive/best_model/best_modeling.ckpt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9WfbB3Dg1Bpq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628659944783,"user_tz":-540,"elapsed":9,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}},"outputId":"a361f042-ca61-4794-eb7b-fc73acada600"},"source":["cls_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"tf_bert_classifier\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","tf_bert_model (TFBertModel)  multiple                  92186880  \n","_________________________________________________________________\n","dropout_37 (Dropout)         multiple                  0         \n","_________________________________________________________________\n","classifier (Dense)           multiple                  5383      \n","=================================================================\n","Total params: 92,192,263\n","Trainable params: 92,192,263\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"50b4CM-XC_5B"},"source":["# TEST"]},{"cell_type":"code","metadata":{"id":"T2IayBqpFefT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628659946691,"user_tz":-540,"elapsed":1912,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}},"outputId":"b7434276-aaee-45c0-ea5e-3980ccdcb96e"},"source":["input_ids = []\n","attention_masks = []\n","token_type_ids = []\n","train_data_labels = []\n","\n","\n","for test_sent in test[\"clear_title\"]: \n","    try:\n","\n","        input_id, attention_mask, token_type_id = bert_tokenizer(test_sent, MAX_LEN)\n","        input_ids.append(input_id)\n","        attention_masks.append(attention_mask)\n","        token_type_ids.append(token_type_id)\n","        \n","    except Exception as e:\n","        print(e)\n","        print(test_sent)\n","        pass\n","\n","\n","test_news_input_ids = np.array(input_ids, dtype=int)\n","test_news_attention_masks = np.array(attention_masks, dtype=int)\n","test_news_type_ids = np.array(token_type_ids, dtype=int)\n","\n","test_news_inputs = (test_news_input_ids, test_news_attention_masks, test_news_type_ids)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-pChpMn67Att","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628659952321,"user_tz":-540,"elapsed":5632,"user":{"displayName":"장준보","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1wOQ7PGKpBDsWEmPjG2rFr7idPoKgnOwi5x4h=s64","userId":"12796992458142872403"}},"outputId":"9bdddead-0ec9-4a6b-d175-f5336bff8b0e"},"source":["cls_model_1 = TFBertClassifier(model_name=model_name,\n","                                                dir_path='bert_ckpt', \n","                                                num_class=NUM_CLASS)\n","cls_model_1.load_weights(os.path.join(DATA_OUT_PATH,'best_modeling.ckpt'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["All PyTorch model weights were used when initializing TFBertModel.\n","\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f54bb42fc10>"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"xL7BSIBh1oU_"},"source":["predictions = cls_model_1.predict(test_news_inputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sHnjm1OQ24Rn"},"source":["pred = np.argmax(predictions, axis = 1)\n","submission.topic_idx = pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mJkO76lD8eGX"},"source":["submission.to_csv(PATH + 'bert.csv',index = False)"],"execution_count":null,"outputs":[]}]}